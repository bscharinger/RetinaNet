{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\n",
      "560529408/560525318 [==============================] - 490s 1us/step\n",
      "560537600/560525318 [==============================] - 490s 1us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "'E:\\\\ML_projects\\\\RetinaNet\\\\data\\\\data.zip'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
    "filepath = \"E:\\ML_projects\\RetinaNet\\data\\data.zip\"\n",
    "keras.utils.get_file(filepath, url)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"E:\\ML_projects\\RetinaNet\\data\\data.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"E:\\ML_projects\\RetinaNet\\data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset coco: could not find data in data\\data\\coco\\2017\\1.1.0. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[0m (train_ds, val_ds), ds_info \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcoco/2017\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvalidation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\\\u001B[39;49m\u001B[38;5;124;43mcoco\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m2017\u001B[39;49m\u001B[38;5;130;43;01m\\\\\u001B[39;49;00m\u001B[38;5;124;43m1.1.0\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:250\u001B[0m, in \u001B[0;36mload.<locals>.decorator\u001B[1;34m(function, unused_none_instance, args, kwargs)\u001B[0m\n\u001B[0;32m    248\u001B[0m name \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m args \u001B[38;5;28;01melse\u001B[39;00m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 250\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    252\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_datasets\\core\\load.py:587\u001B[0m, in \u001B[0;36mload\u001B[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[0;32m    584\u001B[0m as_dataset_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshuffle_files\u001B[39m\u001B[38;5;124m'\u001B[39m, shuffle_files)\n\u001B[0;32m    585\u001B[0m as_dataset_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread_config\u001B[39m\u001B[38;5;124m'\u001B[39m, read_config)\n\u001B[1;32m--> 587\u001B[0m ds \u001B[38;5;241m=\u001B[39m dbuilder\u001B[38;5;241m.\u001B[39mas_dataset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mas_dataset_kwargs)\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_info:\n\u001B[0;32m    589\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m ds, dbuilder\u001B[38;5;241m.\u001B[39minfo\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py:197\u001B[0m, in \u001B[0;36mas_dataset.<locals>.decorator\u001B[1;34m(function, dsbuilder, args, kwargs)\u001B[0m\n\u001B[0;32m    195\u001B[0m metadata \u001B[38;5;241m=\u001B[39m call_metadata\u001B[38;5;241m.\u001B[39mCallMetadata()\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 197\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m function(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    199\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py:628\u001B[0m, in \u001B[0;36mDatasetBuilder.as_dataset\u001B[1;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;66;03m# pylint: enable=line-too-long\u001B[39;00m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mexists(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir):\n\u001B[1;32m--> 628\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    629\u001B[0m       (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: could not find data in \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m. Please make sure to call \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    630\u001B[0m        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_builder.download_and_prepare(), or pass download=True to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    631\u001B[0m        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtfds.load() before trying to access the tf.data.Dataset object.\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m    632\u001B[0m       (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_dir_root))\n\u001B[0;32m    634\u001B[0m \u001B[38;5;66;03m# By default, return all splits\u001B[39;00m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m split \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mAssertionError\u001B[0m: Dataset coco: could not find data in data\\data\\coco\\2017\\1.1.0. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object."
     ]
    }
   ],
   "source": [
    "(train_ds, val_ds), ds_info = tfds.load(\"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"data\\data\\coco\\\\2017\\\\1.1.0\", download=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to E:\\ML_projects\\RetinaNet\\data\\coco\\2017\\1.1.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28ee8adab92349b39a936defffe1eda4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc061f15629a448eac3a4769d99344ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f0846f2414344619d995c51e795eed2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a5256662d3e4cbd9504912ba0edc27f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ada9f4c2ce149f6b1d73ad310ebbf0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling E:\\ML_projects\\RetinaNet\\data\\coco\\2017\\1.1.0.incompleteGYUKTJ\\coco-train.tfrecord*...:   0%|       …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40b9ebb5cb2543aea8fabab9e86b3424"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating validation examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e7f29feeca64baf91aba695b7452eae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling E:\\ML_projects\\RetinaNet\\data\\coco\\2017\\1.1.0.incompleteGYUKTJ\\coco-validation.tfrecord*...:   0%|  …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9017117249734b4694f8b6141f627032"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcc621ecbf2b4204a54866359db85b73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling E:\\ML_projects\\RetinaNet\\data\\coco\\2017\\1.1.0.incompleteGYUKTJ\\coco-test.tfrecord*...:   0%|        …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7294bb6c9504c519768f37d22664344"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset coco downloaded and prepared to E:\\ML_projects\\RetinaNet\\data\\coco\\2017\\1.1.0. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "ds = tfds.load(\"coco/2017\", split=[\"train\", \"validation\"], with_info=True, data_dir=\"E:\\ML_projects\\RetinaNet\\data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}